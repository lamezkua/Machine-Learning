{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Machine Learning Basic Principles 2018 - Data Analysis Project Report**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing different solvers for Logistic Regression in music genre classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project makes use of different solvers to classify music into different genres using logistic regression using classifiers available from the scikit-learn Python library. Classifiers include liblinear, newton-cg, lbfgs, sag, and saga. Various modes for each classifier were also available, with a multinomial and one vs rest mode available for most of the classifiers save liblinear, and two different kind of penalties (l1 and l2). 4361 music samples were used to train all the different classifiers with all configurations of modes and penalities, and the classifier with the best training balanced accuracy and the best training log loss were selected. The best classifier by both metrics turned out to be the liblinear classifier, achieving a balanced accuracy of 0.7183 and a log loss of 0.8660. When used to predict test data, the classifier achieved an accuracy of 0.63219 and a log loss of 0.19278. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Music genre classification is a challenging task, even with machine learning methods (Bahuleyan, 2018). Nonetheless, if properly done, music genre classification certainly could have many uses, including but not limited to recommendation systems where music streaming providers can recommend music of similar genre to what listeners often listen to. It could also possibly be used in early music education, where such a classifier could help young children better distinguish between different genres of music. \n",
    "\n",
    "However, with so many different machine learning methods available, it can be difficult to choose the right method for the task. Thus, in this project, we will explore various different classifiers that all make use of logistic regression and attempt to see which classifier works the best for the task at hand. We will be trying all the logistic regression classifiers available in the scikit-learn Python machine learning library and evaluating their performance.\n",
    "\n",
    "Through this, we hope to expand our knowledge about various logistic regression methods, as well as to learn more about the tools we have at hand today to perform machine learning tasks. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature vectors that we have contain 264 features, which is already a fairly small number of features, which means that computation will not be too heavy. However, due to the large sample size of 4362 for the training data, it would be desirable to reduce the number of features if possible. \n",
    "\n",
    "Therefore, we processed the data and found that for all training and test data, the 216th to 219th elements of the feature vector always had identical values (1e06). We thus removed those values from the feature vector to increase the speed of computation. \n",
    "\n",
    "In terms of class distribution, it can be seen that there is a large amount of labelled data corresponding to label 1 (Pop_Rock) as compared to the other labels. This could lead to fairly accurate predictions for Pop/Rock songs, but could mean that predictions for other genres of music may not be as accurate. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "014a593ce82d342a60d749c7a2c46b7c",
     "grade": true,
     "grade_id": "cell-c3ef844c17cf4a1e",
     "locked": false,
     "points": 1,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn.linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from sklearn.metrics import balanced_accuracy_score, log_loss, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items at indexes [216 217 218 219] for every feature vector are equal for both test and training data\n"
     ]
    }
   ],
   "source": [
    "# Load the data and cleanup\n",
    "train_data = pd.read_csv('train_data.csv', header=None).values\n",
    "test_data = pd.read_csv('test_data.csv', header=None).values\n",
    "# algorithm to search for repeated values in all data\n",
    "# this allows us to remove such repeated values to reduce the size of the feature vector\n",
    "equals = None # array that stores indexes where values are equal\n",
    "for i in range(len(train_data) - 1):\n",
    "    if equals is None:\n",
    "        equals = np.where(np.equal(train_data[i], train_data[i+1]))\n",
    "    else:\n",
    "        equals_i = np.where(np.equal(train_data[i], train_data[i+1]))\n",
    "        equals = np.intersect1d(equals, equals_i)\n",
    "\n",
    "for i in range(len(test_data) - 1):\n",
    "    \n",
    "    if equals is None:\n",
    "        equals = np.where(np.equal(test_data[i], test_data[i+1]))\n",
    "    else:\n",
    "        equals_i = np.where(np.equal(test_data[i], test_data[i+1]))\n",
    "        equals = np.intersect1d(equals, equals_i)\n",
    "print(\"Items at indexes %s for every feature vector are equal for both test and training data\" % equals)\n",
    "# delete repeated elements in feature vectors\n",
    "np.delete(train_data, equals, axis=1)\n",
    "np.delete(test_data, equals, axis=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Analysis of the input data\n",
    "# plot of the class histogram\n",
    "train_labels = pd.read_csv('train_labels.csv', header=None).values\n",
    "plt.hist(train_labels, bins=20)\n",
    "plt.title(\"Class distribution of data\")\n",
    "plt.show()\n",
    "count = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Methods and experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We performed logistic regression in order to classify the data points into the various music genres. As there are many different solvers and parameters for logistic regression using the scikit-learn library, we decided to simply try all of the different solvers before choosing the best ones.\n",
    "\n",
    "Specifically, we chose two solvers out of all the solvers - that is, the solver with the best balanced accuracy, and the solver with the lowest logistic loss. This is so as the two competitions on Kaggle use these accuracy and log loss to judge the solver, and it is reasonable to assume that a solver that has a low training accuracy would have a low test accuracy, and that a solver with a low training logistic loss would have a low test logistic loss.\n",
    "\n",
    "One thing to note is that we use balanced accuracy instead of normal accuracy to determine the best solver. This is so as the data set is extremely unbalanced, and using balanced accuracy allows us to take this into account when deciding which solver performs optimally. Similarly, in the computation of log loss, we also take into account the sample weights.\n",
    "\n",
    "We also tried two different methods to measure the metrics - the first was to use the entire set of training data as both the training and the data used for evaluation, while the second was to split the data into training and test data. While we expected the second one to perform better as it combats overfitting, it turned out that the first method actually performed better in the Kaggle competition and thus that is the method that is used here. We suspect that this is due to the fact that splitting the data into training and test sets reduce the sample size used in training, which impacted the performance of the classifier negatively. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running newton-cg in one vs rest mode with l2 penalty\n",
      "running lbfgs in one vs rest mode with l2 penalty\n",
      "running liblinear in one vs rest mode with l1 penalty\n",
      "running liblinear in one vs rest mode with l2 penalty\n",
      "running sag in one vs rest mode with l2 penalty\n",
      "running saga in one vs rest mode with l1 penalty\n",
      "running saga in one vs rest mode with l2 penalty\n",
      "running newton-cg in multinomial mode with l2 penalty\n",
      "running lbfgs in multinomial mode with l2 penalty\n",
      "running sag in multinomial mode with l2 penalty\n",
      "running saga in multinomial mode with l1 penalty\n",
      "running saga in multinomial mode with l2 penalty\n",
      "Classification complete\n"
     ]
    }
   ],
   "source": [
    "# Trials with ML algorithms\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(train_data, np.transpose(train_labels)[0], random_state=0)\n",
    "import warnings\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "warnings.filterwarnings(\"ignore\", category=ConvergenceWarning)\n",
    "X_train = train_data\n",
    "X_test = X_train\n",
    "y_train = np.transpose(train_labels)[0]\n",
    "y_test = y_train\n",
    "sample_weight = compute_sample_weight(class_weight=None, y=y_test)\n",
    "fast = False # set this to True to only run the best solver\n",
    "# Try one vs rest approach\n",
    "# use all solvers\n",
    "if fast:\n",
    "    solvers = {'liblinear': ['l1']}\n",
    "else:\n",
    "    solvers = {'newton-cg': ['l2'], 'lbfgs': ['l2'], 'liblinear': ['l1','l2'], 'sag': ['l2'], 'saga': ['l1','l2']}\n",
    "results = {}\n",
    "classifiers = {}\n",
    "best_score = 0\n",
    "least_loss = 9999\n",
    "for solver in solvers:\n",
    "    for penalty in solvers[solver]:\n",
    "        print('running %s in one vs rest mode with %s penalty' % (solver, penalty))\n",
    "        classifier = sklearn.linear_model.LogisticRegression(multi_class='ovr', solver=solver, penalty=penalty).fit(X_train, y_train)\n",
    "        y_pred = classifier.predict(X_test)\n",
    "        # save highest score. We use balanced score to account for imbalance in data\n",
    "        score = balanced_accuracy_score(y_test, y_pred, sample_weight)\n",
    "        # calculate log loss and save lowest log loss\n",
    "        probabilities = classifier.predict_proba(X_test)\n",
    "        loss = log_loss(y_test, probabilities, sample_weight=sample_weight, labels=[1,2,3,4,5,6,7,8,9,10])\n",
    "        if score > best_score:\n",
    "            best_score = score\n",
    "            best_accuracy_classifier = classifier\n",
    "            best_accuracy_method = \"%s - %s (%s penalty)\" %(solver, 'one vs rest', penalty)\n",
    "        if loss < least_loss:\n",
    "            least_loss = loss\n",
    "            best_log_loss_classifier = classifier\n",
    "            best_log_loss_method = \"%s - %s (%s penalty)\" %(solver, 'one vs rest', penalty)\n",
    "    \n",
    "        \n",
    "# Try true multinomial approach\n",
    "for solver in solvers:\n",
    "    # liblinear solver does not support true multinomial model\n",
    "    if solver != 'liblinear':\n",
    "        for penalty in solvers[solver]:\n",
    "            print('running %s in multinomial mode with %s penalty' % (solver, penalty))\n",
    "            classifier = sklearn.linear_model.LogisticRegression(multi_class='multinomial', solver=solver, penalty=penalty).fit(X_train, y_train)\n",
    "            y_pred = classifier.predict(X_test)\n",
    "            score = balanced_accuracy_score(y_test, y_pred, sample_weight)\n",
    "            # calculate log loss and save lowest log loss\n",
    "            probabilities = classifier.predict_proba(X_test)\n",
    "            loss = log_loss(y_test, probabilities, labels=[1,2,3,4,5,6,7,8,9,10], sample_weight=sample_weight)\n",
    "            # save highest score\n",
    "            if score > best_score:\n",
    "                best_score = score\n",
    "                best_classifier = classifier\n",
    "                best_method = \"%s - %s (%s penalty)\" %(solver, 'multinomial', penalty)\n",
    "            if loss < least_loss:\n",
    "                least_loss = loss\n",
    "                best_log_loss_classifier = classifier\n",
    "                best_log_loss_method = \"%s - %s (%s penalty)\" %(solver, 'multinomial', penalty)\n",
    "        \n",
    "print(\"Classification complete\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method with highest balanced accuracy is liblinear - one vs rest (l1 penalty) with a balanced accuracy of 0.509821777506\n",
      "This method has an accuracy of 0.718313087325\n",
      "Method with lowest log loss is liblinear - one vs rest (l1 penalty) with balanced log loss of 0.866029224694\n",
      "This method has a log loss of 0.866029224694\n",
      "File writing complete\n"
     ]
    }
   ],
   "source": [
    "# print results and write to file\n",
    "print(\"Method with highest balanced accuracy is %s with a balanced accuracy of %s\" %(best_accuracy_method, best_score))\n",
    "accuracy = best_accuracy_classifier.score(X_train,y_train)\n",
    "print(\"This method has an accuracy of %s\" % accuracy)\n",
    "print(\"Method with lowest log loss is %s with balanced log loss of %s\" %(best_log_loss_method, least_loss))\n",
    "probabilities = best_log_loss_classifier.predict_proba(X_test)\n",
    "loss = log_loss(y_test, probabilities)\n",
    "print(\"This method has a log loss of %s\" % loss)\n",
    "predicted = best_accuracy_classifier.predict(test_data)\n",
    "probabilities = best_log_loss_classifier.predict_proba(test_data)\n",
    "with open('output_accuracy.csv', 'w') as f:\n",
    "    f.write(\"Sample_id,Sample_label\\n\")\n",
    "    for i in range(len(predicted)):\n",
    "        f.write(\"%s,%s\\n\" % (i+1, predicted[i]))\n",
    "with open('output_logloss.csv', 'w') as f:\n",
    "    f.write(\"Sample_id,Class_1,Class_2,Class_3,Class_4,Class_5,Class_6,Class_7,Class_8,Class_9,Class_10\\n\")\n",
    "    for i in range(len(probabilities)):\n",
    "        arr_string = np.char.mod('%f', probabilities[i])\n",
    "        f.write(str(i+1) + \",\" + \",\".join(arr_string) + \"\\n\")\n",
    "print(\"File writing complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As it turned out, the best classifier for both balanced accuracy and log loss was the liblinear classifier with l1 penalty. The training accuracy of this classifier was 0.7183 and the log loss was 0.8660. As we would expect, it performed worse on accuracy in the kaggle competition, with an accuracy of 0.63219. This is due to the fact that this classifier was trained specifically to fit the training data, and so we would expect it to perform better on the training data. However the classifier surprisingly performed better for log loss, with a log loss of 0.19278. \n",
    "\n",
    "The confusion matrix for this classifier is shown below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1981,   94,   19,   24,   11,   24,    5,   16,    3,    1],\n",
       "       [ 165,  412,   14,   11,    5,   10,    0,    0,    1,    0],\n",
       "       [  41,   22,  247,    3,    1,    8,    1,    0,    3,    0],\n",
       "       [  56,   23,    2,  155,    3,    7,    2,    4,    0,    1],\n",
       "       [ 116,   11,    9,    8,   49,    6,    2,    3,    7,    3],\n",
       "       [  94,    7,   15,   10,    8,  117,    2,    2,    4,    1],\n",
       "       [  80,   12,   10,    4,    4,    6,   19,    3,    2,    1],\n",
       "       [ 104,    1,    0,    1,    1,    3,    1,   84,    0,    0],\n",
       "       [  10,    7,   13,    0,    4,    1,    0,    0,   57,    0],\n",
       "       [  47,    0,    1,    6,    1,    0,    3,    4,    0,   24]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = best_accuracy_classifier.predict(X_test)\n",
    "confusion_matrix(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Discussion/Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Both accuracy and log loss are not ideal for an unbalanced data set. This is so as with the huge imbalance in the data set (large amount of label 1), it is more likely for the model to predict that a sample is of type 1, which may skew accuracy statistics since the training accuracy would be very high since there are many samples of type 1, but the actual accuracy when used with other data may decrease if there are many samples of other types. Similarly for log loss, it is not the best metric as it weighs each type of misclassification equally (Nowling, 2016). In this case, as there are a large number of type 1 music samples in the training data, even if hypothetically all other samples were wrongly predicted as type 1, the log loss would still be low due to the low number of non-type 1 samples. Thus, accuracy and log loss, on their own, are not ideal for measuring the performance of the classifier.\n",
    "\n",
    "To counteract this problem as mentioned in section 3, we took into account the sample weights when computing the accuracy and log loss measures. This means that correct identification of type 1 samples will be given a lower weightage than correct identification of samples of other types in the case of accuracy, while misclassifying a sample as type 1 is given more weightage in the case of log loss. This helps to combat the inherent problems of using log loss and accuracy for an imbalanced data set. \n",
    "\n",
    "Through this study, we were able to determine the best logistic regression model to determine the genre of music, which turned out to be the liblinear classifier with l1 penalty. However, there is still much work to be done. There are many other machine learning methods such as neural networks that are not explored in this project, and which may perform better than logistic regression for this particular task.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bahuleyan, H. (2018). Music Genre Classification using Machine Learning Techniques. Retrieved from https://arxiv.org/pdf/1804.01149.pdf. \n",
    "\n",
    "Nowling, R. (2016, August 26). Pitfalls When Working With Imbalanced Data Sets. Retrieved October 30, 2018, from http://rnowling.github.io/data/science/2016/08/26/imbalanced-dataset-pitfalls.html "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
