{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5736a37472c7741216fd29915c748ecb",
     "grade": false,
     "grade_id": "cell-32738734cf6f1a4f",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "# Machine Learning: Basic Principles 2018\n",
    "# Clustering\n",
    "\n",
    "### Learning goals \n",
    "\n",
    "In this exercise you will learn how to organize a large number of data points into coherent groups (clusters) using clustering methods. In particular, we consider the hard clustering method __k-means__ and a soft clustering method which is motivated by probabilistic __Gaussian mixture models__.\n",
    "### Exercise Contents\n",
    "\n",
    "1. Introduction\n",
    "2. Data\n",
    "3. Exercise\n",
    "    - The exercise consists of 3 tasks. Carefully read the task descriptions and instructions\n",
    "    - 3.1 Hard clustering with k-means\n",
    "    - 3.2 Handling local minima with k-means\n",
    "    - 3.3 Soft clustering with Gaussian mixture models (GMMs)\n",
    "\n",
    " \n",
    "    \n",
    "  \n",
    "### Keywords\n",
    "`hard clustering`, `soft clustering`, `k-means`, `Gaussian mixture model (GMM)`, `unsupervised learning`.\n",
    "\n",
    "##  1. Introduction\n",
    "<a id=\"intro\"></a>\n",
    "\n",
    "On a high level, clustering is the task of dividing a (unlabeled) dataset $\\mathbf{X} = \\{ \\mathbf{x}^{(i)} \\}_{i=1}^{N}$, with $N$ data points $\\mathbf{x}^{(i)} \\in \\mathbb{R}^{d}$, into a small number of groups or \"clusters\" $\\mathcal{C}_{1},\\ldots,\\mathcal{C}_{k}$. Each cluster represents a subset of data points such that data points belonging to the same cluster are more similar to each other than to data points from another cluster. In contrast to regression and classification problems considered in earlier exercises, clustering methods do not require labeled data and can be applied to datasets whose data points $\\mathbf{z}^{(i)}$ are characterized solely by its features $\\mathbf{x}^{(i)}$. Therefore clustering methods are referred to as **unsupervised** machine learning methods.\n",
    "\n",
    "There are two main flavors of clustering methods: \n",
    "\n",
    "* hard clustering methods  \n",
    "* soft clustering methods\n",
    "\n",
    "\n",
    "Hard clustering methods assign each data point to one and only one cluster. In contrast, soft-clustering methods assign each data point to several different clusters with varying probabilities.\n",
    "\n",
    "We will apply one popular method for hard clustering (k-means) and one popular method for soft clustering (which is based on a probabilistic Gaussian mixture model) to a real-life application. In particular, consider you are running a Cafe in Helsinki and you want to segment customers in order to design a new marketing strategy for the upcoming summer. Such a customer segmentation can be done efficiently using clustering methods.\n",
    "\n",
    "### 2. Data\n",
    "<a id=\"steps_k-means\"></a>\n",
    "\n",
    "The Cafe owner wants to know if the customers can be grouped into subgroups of similar costumers. Based on this it might be useful to employ different marketing strategies targeting each group individually. So far the only information the owner collected was the age of the customers (there was a nasty incident in the past and now everyone is asked to show the ID when ordering alcoholic beverages) and the amount of money they spent (which can be recorded conveniently for card payments). The recordings are stored in the file \"data.csv\".\n",
    "\n",
    "In the file \"data.csv\" you will find 400 rows, each of which contains the features for one data point (customer). The first column contains the age of the customers and the second column contains the amount of money they spent at the bar. Let us read it into a $400 \\times 2$ numpy array and then visualize it using a scatter plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "b1b020a667788e93e5eb0f11163e0997",
     "grade": false,
     "grade_id": "cell-25bd30950c8c3bbe",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "#import the needed libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "import numpy as np\n",
    "\n",
    "## Choosing nice colors for plot\n",
    "#if you want to plot for k>3, extend these lists of colors\n",
    "cmpd = ['orangered','dodgerblue','springgreen']\n",
    "cmpcent = ['red','darkblue','limegreen']\n",
    "\n",
    "\n",
    "#read in data from the csv file\n",
    "df = pd.read_csv(\"data.csv\")\n",
    "data=df.as_matrix()\n",
    "\n",
    "#display first 5 rows, to get a feeling for the data\n",
    "display(df.head(5))\n",
    "\n",
    "\n",
    "def plotting(data, centroids=None, clusters=None):\n",
    "    #this function will later on be used for plotting the clusters and centroids. But now we use it to just make a scatter plot of the data\n",
    "    #Input: the data as an array, cluster means (centroids), cluster assignemnts in {0,1,...,k-1}   \n",
    "    #Output: a scatter plot of the data in the clusters with cluster means\n",
    "    plt.figure(figsize=(5.75,5.25))\n",
    "    plt.style.use('ggplot')\n",
    "    plt.title(\"Data\")\n",
    "    plt.xlabel(\"feature $x_1$: customers' age\")\n",
    "    plt.ylabel(\"feature $x_2$: money spent during visit\")\n",
    "\n",
    "    alp = 0.5             #data alpha\n",
    "    dt_sz = 20            #data point size\n",
    "    cent_sz = 130         #centroid sz\n",
    "    \n",
    "    if centroids is None and clusters is None:\n",
    "        plt.scatter(data[:,0], data[:,1],s=dt_sz,alpha=alp ,c=cmpd[0])\n",
    "    if centroids is not None and clusters is None:\n",
    "        plt.scatter(data[:,0], data[:,1],s=dt_sz,alpha=alp, c=cmpd[0])\n",
    "        plt.scatter(centroids[:,0], centroids[:,1], marker=\"x\", s=cent_sz, c=cmpcent)\n",
    "    if centroids is not None and clusters is not None:\n",
    "        plt.scatter(data[:,0], data[:,1], c=[cmpd[i] for i in clusters], s=dt_sz, alpha=alp)\n",
    "        plt.scatter(centroids[:,0], centroids[:,1], marker=\"x\", c=cmpcent, s=cent_sz)\n",
    "    if centroids is None and clusters is not None:\n",
    "        plt.scatter(data[:,0], data[:,1], c=[cmpd[i] for i in clusters], s=dt_sz, alpha=alp)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "#plot the data\n",
    "plotting(data)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "58f3a32d56fd58c8a9c2386dcda6edf4",
     "grade": false,
     "grade_id": "cell-f1d445ffe42b0659",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3. Exercise\n",
    "The actual exercise starts from here and it has been divided in 3 tasks:\n",
    "* 3.1 **Hard clustering with k-means** \n",
    "* 3.2 **Handling local minima with k-means**\n",
    "* 3.3 **Soft clustering with Gaussian mixture models (GMMs)**\n",
    "\n",
    "Your task is to fill in `### STUDENT TASK ###` in each step.\n",
    "\n",
    "## 3.1 Hard clustering with k-means\n",
    "<a id=\"kmeans\"></a>\n",
    "\n",
    "A popular method for hard clustering is the k-means algorithm which takes as input a list of data points $\\mathbf{x}^{(1)},...,\\mathbf{x}^{(N)} \\in \\mathbb{R}^{d}$ and groups them into $k$ non-overlapping clusters $\\mathcal{C}_{¡},\\ldots,\\mathcal{C}_{k}$. Each cluster $\\mathcal{C}_{c}$ is characterized by its cluster mean $\\mathbf{m}^{(c)} \\in \\mathbb{R}^{d}$. As a hard-clustering method, k-means assigns each data point $\\mathbf{x}^{(i)}$ to exactly one cluster whose index we denote as $y^{(i)} \\in 1,...,k$. We can interpret the cluster assignments $y^{(i)}$ as (hidden) labels of the data points. However, we do not have access to the cluster assignment of any data point but rather have to estimate them based on the internal or intrinsic structure of the data set $\\mathbf{x}^\n",
    "{(1)}, \\ldots, \\mathbf{x}^{(N)}$.\n",
    "\n",
    "\n",
    "K-means can be summarized as follows:\n",
    "\n",
    "* Choose initial cluster means $ \\mathbf{m}^{(1)},...,\\mathbf{m}^{(k)}$\n",
    "\n",
    "* Repeat until stopping condition is reached:  \n",
    "\n",
    "    * Assign each datapoint to the cluster whose mean is nearest. \n",
    "    \n",
    "    I.e. for all $i=1,...,N$, do  $$ y^{(i)} = \\underset{c'}{\\operatorname{argmin}} \\|\\mathbf{x}^{(i)} - \\mathbf{m}^{(c')}\\|^2 $$\n",
    "    \n",
    "    * Update the new cluster means by calculating the average of the points in the cluster. \n",
    "    \n",
    "    I.e. for all $c=1,...,k$, set \n",
    "    \\begin{equation*}\n",
    "    \\mathbf{m}^{(c)} = \\frac{1}{\\mid\\{i: y^{(i)}= c\\}\\mid}{\\sum_{i: y^{(i)}= c}\\mathbf{x}^{(i)}}     \\label{mean}\n",
    "    \\tag{1}\n",
    "    \\end{equation*}\n",
    "    where $\\{i: y^{(i)}= c\\}$ represents the set of datapoints belonging to cluster c and $\\mid\\{i: y^{(i)}= c\\}\\mid$ the number of datapoints belonging to cluster c.  \n",
    "    \n",
    "    Note: Do this only for the clusters with at least one member, those with no members keep the old mean. \n",
    "\n",
    "\n",
    "Thus, k-means consists of 4 simple steps:\n",
    "\n",
    "* __Step 1 - Initialize cluster means.__\n",
    "* __Step 2 - Update the cluster assignments: assign datapoints to the nearest cluster means.__\n",
    "* __Step 3 - Update the cluster means by computing the mean (average) of the data points assigned to a particular cluster.__\n",
    "* __Step 4 - If not finished go to step 2.__\n",
    "\n",
    "The working of $k$-means is best understood by walking through an example. To this end, we apply $k$-means to the Café customer data. \n",
    "\n",
    "\n",
    "### Step 1: Initialize cluster means\n",
    "\n",
    "We first initialize the cluster means, also called centroids. The most common way is to do this by randomly choosing points between the minimum and maximum values of the data. However, here we select predefined data points, in order to have no randomness. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "90fe1f96cf9894a1db654e9cb179fec1",
     "grade": true,
     "grade_id": "cell-9cbc1eb4467c21d6",
     "locked": true,
     "points": 0,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def select_centroids(data, k, random_seed=1):   \n",
    "    #INPUT: N x d data array, k number of clusters. \n",
    "    #OUTPUT: k x d array of k randomly assigned mean vectors with d dimensions.\n",
    "    \n",
    "    # Random seed will generate exactly same \"random\" values for each execution.\n",
    "    # This will ensure similar results between students and avoid confusion.\n",
    "    np.random.seed(seed=random_seed)\n",
    "    \n",
    "    centroids = np.zeros((k, data.shape[1]))\n",
    "    for i in range(data.shape[1]):\n",
    "        \n",
    "        centroids[:,i] = np.random.uniform(np.min(data[:,i]), \n",
    "                                           np.max(data[:,i]), \n",
    "                                           size = (k))\n",
    "    return centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "578a82a7f6c241cfd02d2fbd8c456cbb",
     "grade": false,
     "grade_id": "cell-5e4b00f71ee5e896",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Run the code below with the number of clusters, k, being 2. You should see a plot of the data points and the initial means as crosses. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1226bd26af0b3553b10890288395a75f",
     "grade": true,
     "grade_id": "cell-39df4a774343b28d",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "centroids = select_centroids(data, 2)\n",
    "plotting(data, centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b29d970908c391a5d3d159f9c4797db9",
     "grade": false,
     "grade_id": "cell-16c8205d33449e10",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Step 2: Cluster assignment update: Assign each datapoint to the cluster of nearest cluster mean\n",
    "\n",
    "In this step your task is to assign each data point $\\mathbf{x}^{(i)}$ to the cluster $c$ with the nearest cluster mean $\\mathbf{m}^{(c)}$. This distance is measured by the Euclidean distance $\\| \\mathbf{x}^{(i)} - \\mathbf{m}^{(c)} \\|$. This will result in a vector of length $N$ whose entries are the cluster assignments $y^{(i)}$ for each point.\n",
    "\n",
    "#### Tasks\n",
    "- Implement a Python function `assign_points()` which takes as inputs the data and centroids (cluster means). It should return a vector, named _clusters_, whose entries are the cluster assignments $y^{(i)}$ for each point. \n",
    "    - You can use `np.linalg.norm()` to get the Euclidean distance. \n",
    "    - You can use `np.argmin()` to get the indices of the minimum values along an axis in an array.\n",
    "    - Note: Remember that indexing in Python starts from 0, hence it might be easier to number the clusters from 0...k-1, but it is up to you to decide :)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "250c0460da2e6ee19e6074c925e68af0",
     "grade": false,
     "grade_id": "cell-440011bf66fe1cdb",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def assign_points(data, centroids):     \n",
    "    #INPUT: N x d data array, k x d centroids array.\n",
    "    #OUTPUT: N x 1 array of cluster assignments in {0,...,k-1}.\n",
    "    \n",
    "    clusters = np.zeros(data.shape[0],dtype=np.int32)\n",
    "    \n",
    "    ### STUDENT TASK ###\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "1be994ce2106066c22e6564812e6c1e5",
     "grade": false,
     "grade_id": "cell-499e32db19e29a2b",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's look at what clusters we have so far.\n",
    "\n",
    "Run the cell below for that. The datapoints with the same color, belong to the same cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "7c22e3d465bc6ed9c0cff9a28b0d5e44",
     "grade": true,
     "grade_id": "cell-5eb4039d91e99e5d",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "clusters = assign_points(data, centroids)\n",
    "plotting(data, centroids, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "5f47a78e73a36f0f40f6d551bc6a2963",
     "grade": false,
     "grade_id": "cell-2c56b5c23b2c5f9a",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Does it make sense? Yes? Awesome! Let's proceed further.\n",
    "\n",
    "\n",
    "### Step 3: update cluster means\n",
    "\n",
    "We now want to update the cluster means $\\mathbf{m}^{(c)}$ of the data points that were assigned to cluster c during the previous step.\n",
    "\n",
    "#### Tasks\n",
    "- Implement the Python function `move_centroids()` which takes as input the data, old centroids and cluster assignments. It should return the new centroids. \n",
    "    - You can use `np.mean()` to compute the mean (see Eq. (1)) of all points belonging to one cluster. \n",
    "    - You need to keep in mind that there might be a situation when none of the points were assigned to a cluster. If this is the case, you should assign the old centroid as new centroid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "737c5338ce465b56e6d82f7654b20e5c",
     "grade": false,
     "grade_id": "cell-4e8eb12d9306e457",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def move_centroids(data, old_centroids, clusters):\n",
    "    #INPUT:  N x d data array, k x d centroids array, N x 1 array of cluster assignments\n",
    "    #OUTPUT: k x d array of relocated centroids\n",
    "    \n",
    "    new_centroids = np.zeros(old_centroids.shape)\n",
    "    ### STUDENT TASK ###\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return new_centroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "aacaee9d7f14993d7c9fbe1864c57062",
     "grade": false,
     "grade_id": "cell-c965bd0ae078b247",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's see if the cluster means moved. Run the cell below for that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "954942fb2fb215ee7dc0d5e31e6577f8",
     "grade": true,
     "grade_id": "cell-1a4c17960789a6a4",
     "locked": true,
     "points": 0,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "new_centroids = move_centroids(data, centroids,clusters)\n",
    "plotting(data, new_centroids, clusters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c38c36f26ac200d143d5793e15fe02b0",
     "grade": false,
     "grade_id": "cell-0a43a89568c63044",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Are the cluster means in the middle of the clusters now? Yes? Nice! We're ready to assemble the whole algorithm now!\n",
    "\n",
    "### Step 4 - Repeat steps 2 and 3 until convergence\n",
    "\n",
    "Now it is time to combine the functions you have created so far into the final algorithm. The new part here is the loop where you repeat steps 2 and 3 until the stopping criterion is fulfilled. Here we are using as stopping criterion a fixed number of iterations. But there are many ways to define a stopping criterion, for example, by checking if the clusters are changing.  \n",
    "\n",
    "Tasks\n",
    "- In the Python function `k_means()` call the functions that were created in step 1 to 3 in order to get the full k-means algorithm. It should output the final centroids and clusters after the given number of iterations, `num_iters`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "eee8f1735dbc3b9f8e82560ce8bc498c",
     "grade": false,
     "grade_id": "cell-bc562c2dbdefd3dd",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "def k_means(data, k, random_seed=1, num_iters=10,plot=True):\n",
    "    #INPUT: N x d data array, k number of clusters, number of iterations, boolean plot.\n",
    "    #OUTPUT: N x 1 array of cluster assignments.\n",
    "    \n",
    "    ### STUDENT TASK ###\n",
    "    #step 1\n",
    "    #centroids = ...\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    \n",
    "        #loop for steps 2 and 3\n",
    "    for i in range(num_iters):\n",
    "        ### STUDENT TASK ###\n",
    "        #step 2\n",
    "        #clusters = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    "        \n",
    "        #plotting\n",
    "        if plot==True and i<3:\n",
    "            plotting(data,centroids,clusters)\n",
    "        \n",
    "        ### STUDENT TASK ###\n",
    "        #step 3\n",
    "        #centroids = ...\n",
    "        # YOUR CODE HERE\n",
    "        raise NotImplementedError()\n",
    " \n",
    "    return centroids,clusters\n",
    "\n",
    "\n",
    "centroids,clusters = k_means(data, 2)\n",
    "print(\"The final cluster mean values are:\",centroids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "06be023e927acec89187417cfac3a9eb",
     "grade": false,
     "grade_id": "cell-75969c53d25b1a55",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3.2. Handling local minima in k-means\n",
    "<a id=\"local\"></a>\n",
    "\n",
    "As introduced in the course book, the k-means algorithm aims at minimizing the __empirical risk__: \n",
    "\n",
    "$$\\mathcal{E}  ( \\{\\mathbf{m}^{(c)}\\}_{c=1}^{k},\\{y^{(i)}\\}_{i=1}^{N} \\mid \\{\\mathbf{x}^{(i)}\\}_{i=1}^{N} )\n",
    "=\\frac{1}{N} \\sum_{i=1}^{N} {\\left\\|\\mathbf{x}^{(i)}-\\mathbf{m}^{(y^{(i)})}\\right\\|^2}\n",
    "$$\n",
    "\n",
    "Since the empirical risk is a highly non-convex function of the cluster means and assignments, the k-means method will sometimes get trapped in a local minimum.\n",
    "\n",
    "It is therefore useful to run k-means several times with different initializations for the cluster means and choose the cluster assignment that yields the smallest empirical risk. \n",
    "\n",
    "Tasks\n",
    "- Implement a Python function `empirical_risk()` which takes as input the data, clusters and centroids and returns the empirical risk. \n",
    "- Then run the cell below, which exectues k-means 50 times, each time with a different initialization for cluster means, and plots the best and worst cluster division for k=3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "516433056a51b3ca9630f100e68619e5",
     "grade": false,
     "grade_id": "cell-eb0e04f8680f836b",
     "locked": false,
     "schema_version": 2,
     "solution": true
    }
   },
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress\n",
    "from IPython.display import display\n",
    "\n",
    "def empirical_risk(data, clusters, centroids):  \n",
    "    #INPUT: N x d data array, k x d array of k mean vectors (centroids), \n",
    "    #       N x 1 array of cluster assignments.\n",
    "    #OUTPUT: value of empirical risk\n",
    "    \n",
    "    ### STUDENT TASK ###\n",
    "    # YOUR CODE HERE\n",
    "    raise NotImplementedError()\n",
    "    return risk\n",
    "\n",
    "def new_k_means(data, k,plot=True):  \n",
    "    # This will display a progress bar during k-mean execution\n",
    "    f = IntProgress(description=f'KM (k={k}):',min=0, max=50)\n",
    "    display(f)\n",
    "    \n",
    "    \n",
    "    # initializing the array where we collect all cluster assignments  \n",
    "    cluster_collection = np.zeros((50, data.shape[0]),dtype=np.int32)\n",
    "    # initializing the array where we collect all risk values \n",
    "    risk_collection = np.zeros(50)\n",
    "    \n",
    "    for i in range(50):\n",
    "        f.value +=1\n",
    "        centroids,clusters=k_means(data,k,random_seed=i,plot=False)\n",
    "        risk_collection[i] = empirical_risk(data, clusters, centroids)\n",
    "        cluster_collection[i,:] = clusters\n",
    "        \n",
    "    #find the best cluster assignment and print the lowest found empirical risk\n",
    "    min_ind = np.argmin(risk_collection)\n",
    "    max_ind=np.argmax(risk_collection)\n",
    "    if plot==True:\n",
    "        print(\"Cluster division with lowest empirical risk\")\n",
    "        plotting(data,clusters=cluster_collection[min_ind,:])\n",
    "        print(\"Cluster division with highest empirical risk\")\n",
    "        plotting(data,clusters=cluster_collection[max_ind,:])\n",
    "    \n",
    "        print('min empirical risk is ', np.min(risk_collection))\n",
    "    \n",
    "    #Let's remove progress bar\n",
    "    f.close()\n",
    "    return cluster_collection[min_ind,:],risk_collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "586b3f9986e7fe0d6e8f68ec76a091c6",
     "grade": true,
     "grade_id": "cell-4cc22f90251bb1a5",
     "locked": true,
     "points": 0,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "best_cluster,risk=new_k_means(data,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "0cfdf120bb92b52b2fbbddf7a5e62c46",
     "grade": false,
     "grade_id": "cell-4241c9a7fb62aa51",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Choosing the best number of clusters\n",
    "We often don't know what the best number of clusters is. One way to find this out is by running the k-means algorithm for several numbers of clusters and choosing the number of clusters which has the best balance between a low empirical risk and a low complexity (i.e. small number of clusters). \n",
    "Run the code below to get a plot of the number of clusters vs the empirical risk. What do you think would be the optimal number of clusters for this dataset?\n",
    "Note: this will take some time to finish running"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "630a96dc53a54bab055952ebf8885da0",
     "grade": true,
     "grade_id": "cell-32dccc160d1d3eff",
     "locked": true,
     "points": 1,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "risks=np.zeros(8)\n",
    "for i in range(0,8):\n",
    "    best_cluster,risk=new_k_means(data,i+1,plot=False)\n",
    "    risks[i]=np.mean(risk)\n",
    "\n",
    "fig=plt.figure(figsize=(8,6))\n",
    "plt.plot(range(1,9),risks)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Empirical risk')\n",
    "plt.title(\"The number of clusters vs the empirical risk\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "b2af10550aea4310f30a126497d2cb0a",
     "grade": false,
     "grade_id": "cell-563bae1795984110",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "## 3.3 Soft clustering with Gaussian Mixture Models (GMM) \n",
    "<a id=\"GMM\"></a>\n",
    "\n",
    "The information provided by $k$ means is rather coarse-grained: even if two data points belong to the same cluster, their location within the cluster might be very different. Let us now consider soft-clustering methods which provide a more fine-grained information about the cluster structure of a data set. \n",
    "\n",
    "Considering again the customer segmentation for a Cafe business, we might like to have some measure for the extend (or degree) by which a customer belongs to various groups. This is a soft-clustering problem where we associate each data point $\\mathbf{x}^{(i)}$(which represents a particular customer) with a membership-vector $\\mathbf{y}^{(i)}= (y^{(i)}_1,...,y^{(i)}_k) \\in [0,1]^k$ whose entry $y^{(i)}_c$ is the degree by which we assign $\\mathbf{x}^{(i)}$ to cluster $\\mathcal{C}_c$. \n",
    "\n",
    "A principled approach to obtaining a soft-clustering method is based on interpreting the data points $\\mathbf{x}^{(i)}$ as realizations of a random vector $\\mathbf{x}$ with some underlying probability distribution $p(\\mathbf{x})$. In particular, we can represent a cluster $\\mathcal{C}_{c}$ by a Gaussian distribution with mean vector $\\mathbf{m}^{(c)}$ and covariance matrix $\\mathbf{C}^{(c)}$. The probability density function of such a Gaussian distribution is denoted \n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{N}(\\mathbf{x}^{(i)} ; \\mathbf{m}^{(c)}, \\mathbf{C}^{(c)}) = \\frac{1}{\\sqrt{{\\rm det} \\big(2 \\pi \\mathbf{C}^{(c)}\\big)}} {\\rm exp } \\bigg( - \\big(\\mathbf{x}^{(i)} - \\mathbf{m}^{(c)} \\big)^{T} \\big(\\mathbf{C}^{(c)}\\big)^{-1}  \\big(\\mathbf{x}^{(i)} - \\mathbf{m}^{(c)} \\big)\\bigg).\n",
    "\\end{equation}\n",
    "\n",
    "The overall probability distribution is then obtained as a **Gaussian mixture** \n",
    "\n",
    "\\begin{equation}\n",
    "p(\\mathbf{x}) = p_{1} \\mathcal{N}(\\mathbf{x} ; \\mathbf{m}^{(1)}, \\mathbf{C}^{(1)}) + p_{2} \\mathcal{N}(\\mathbf{x} ; \\mathbf{m}^{(2)}, \\mathbf{C}^{(2)})\n",
    "\\end{equation}\n",
    "\n",
    "with coefficients $p_{1},p_{2} \\geq 0$ satisfying $p_{1} + p_{2} =1$. The coefficient $p_{1}$ is the probability that a data point is drawn from cluster $\\mathcal{C}_{1}$ and, similarly, $p_{2}$ is the probability that a data point is drawn from cluster $\\mathcal{C}_{2}$. Note that the distribution $p(\\mathbf{x})$ depends (is parametrized by) the coefficients $p_{1},p_{2}$, the cluster means $\\mathbf{m}^{(1)},\\mathbf{m}^{(2)}$ \n",
    "and the covariance matrices $\\mathbf{C}^{(1)},\\mathbf{C}^{(2)}$. This is when we set the number of clusters to 2, but just as in k-means we can also set it to another number. \n",
    "\n",
    "Under the probabilistic model $p(\\mathbf{x})$, the degree $y_{c}^{(i)}$ of a data point $\\mathbf{x}^{(i)}$ belonging to cluster $\\mathcal{C}_{c}$ can be defined as the probability that $\\mathbf{x}^{(i)}$ is generated (drawn) from the Gaussian distribution associated with $\\mathcal{C}_{c}$:  \n",
    "\n",
    "$$\\mathbf{y}^{(i)}_c = \\frac{p_{c} \\mathcal{N}(\\mathbf{x}^{(i)} ; \\mathbf{m}^{(c)}, \\mathbf{C}^{(c)})}{\\sum_{c'=1}^k p_{c'} \\mathcal{N}(\\mathbf{x}^{(i)} ; \\mathbf{m}^{(c')}, \\mathbf{C}^{(c')})} $$\n",
    "\n",
    "After determining the degrees of belonging, we can then update or guess (estimate) the cluster probabilities $p_{1},p_{2}$, cluster means $\\mathbf{m}^{(1)}, \\mathbf{m}^{(2)}$ and covariance matrices $\\mathbf{C}^{(1)},\\mathbf{C}^{(2)}$.\n",
    "\n",
    "In summary this algorithm consists of 4 steps (just as k-means):\n",
    "\n",
    "* __Step 1 - Initialize the cluster parameters. This are the means and covariances for every cluster.__\n",
    "* __Step 2 - Update the degree of data point \\mathbf{x}^{(i)} belonging to cluster c.__\n",
    "* __Step 3 - Update the cluster parameters, i.e. the means and covariances.__\n",
    "* __Step 4 - Put steps 2 and 3 in a loop__\n",
    "\n",
    "So lets go through these steps one by one and compose the algorithm. \n",
    "\n",
    "### Step 1: Initialize the cluster parameters\n",
    "\n",
    "We first choose an initial mean, covariance matrix and coefficient for each cluster. The means are initialized randomly,as initialization for the covariance matrix we take the identity matrix and all coefficients are set to 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "533e2d7212ae9e7b76ca89407baa7f63",
     "grade": true,
     "grade_id": "cell-54368c23a2ce5175",
     "locked": true,
     "points": 0,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def initialize_parameters(data, k,random_seed=1):\n",
    "    \n",
    "    # Random seed will generate exactly same \"random\" values for each execution.\n",
    "    # This will ensure similar results between students and avoid confusion.\n",
    "    np.random.seed(seed=random_seed)\n",
    "    \n",
    "    means = np.zeros((k, data.shape[1]))\n",
    "    for i in range(data.shape[1]):\n",
    "        means[:,i] = np.random.uniform(np.min(data[:,i]), \n",
    "                                           np.max(data[:,i]), \n",
    "                                           size = (k)) \n",
    "    covariances=np.zeros((k,data.shape[1],data.shape[1]))\n",
    "    \n",
    "    for f in range(k):\n",
    "        covariances[f]=np.identity(data.shape[1])\n",
    "    coefficients=np.ones(k)*0.5\n",
    "    return means, covariances,coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "2a78fcf2d635c08965d5f744c7d519d1",
     "grade": true,
     "grade_id": "cell-1d31463f3e548c97",
     "locked": true,
     "points": 0,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def plot_GMM(data,means,covariances,k,cluster_vectors=None):\n",
    "    if cluster_vectors is None:\n",
    "        plt.scatter(data[:,0], data[:,1], s=13,alpha=0.5)\n",
    "    else:\n",
    "        clusters = np.argmax(cluster_vectors,axis=0)\n",
    "        plt.scatter(data[:,0], data[:,1], c=[cmpd[i] for i in clusters], s=13,alpha=0.5)\n",
    "\n",
    "    #Visualization of results\n",
    "    x_plot = np.linspace(19,35, 100)\n",
    "    y_plot = np.linspace(0,12, 100)\n",
    "\n",
    "    for i in range(k):\n",
    "        x_mesh, y_mesh = np.meshgrid(x_plot, y_plot)\n",
    "        z= plt.mlab.bivariate_normal(x_mesh, y_mesh, np.sqrt(covariances[i,0, 0]), \\\n",
    "                                    np.sqrt(covariances[i,1, 1]), means[i,0], means[i,1],covariances[i,0,1])\n",
    "        plt.contour(x_mesh , y_mesh , z,4,colors=cmpcent[i],alpha=0.5)\n",
    "        plt.scatter( [means[i,0]], [means[i,1]], marker='x',c=cmpcent[i])\n",
    "\n",
    "    plt.title(\"Soft clustering with GMM\")\n",
    "    plt.xlabel(\"feature x_1: customers' age\")\n",
    "    plt.ylabel(\"feature x_2: money spent during visit\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "3e858c9ca2e086dc1025fa67e8f96ab5",
     "grade": false,
     "grade_id": "cell-edaaff4b62af543c",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Let's calculate and plot the initializations. The circles indicate the probability distribution, based on the covariances. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "c4d0fef36ae17fba1da711596713ed2d",
     "grade": true,
     "grade_id": "cell-4f2ba92d6a52d434",
     "locked": true,
     "points": 0,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "means,covariances,coefficients=initialize_parameters(data,3)\n",
    "plot_GMM(data,means,covariances,3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "62ca05c1889faf390bc055fca9e96a7c",
     "grade": false,
     "grade_id": "cell-08aac0e23b00115e",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Step 2: Assign a probability for each datapoint belonging to each cluster\n",
    "\n",
    "We now want to calculate for each datapoint the probability it belongs to each cluster. The probability of belonging to a cluster, is also called the degree of belonging. We can calculate this by the following formula:\n",
    "\n",
    "$$y^{(i)}_c = \\frac{p_{c} \\mathcal{N}(\\mathbf{x}^{(i)} ; \\mathbf{m}^{(c)}, \\mathbf{C}^{(c)})}{\\sum_{c'=1}^k p_{c'} \\mathcal{N}(\\mathbf{x}^{(i)} ; \\mathbf{m}^{(c')} , \\mathbf{C}^{(c')})} $$\n",
    "\n",
    "Here, $\\mathcal{N}(\\mathbf{x}^{(i)} ; \\mathbf{m}^{(c)}, \\mathbf{C}^{(c)})$ is the probability density function (or pdf for short). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "53bd8c91093593f4805994507087b60d",
     "grade": true,
     "grade_id": "cell-cb22ee20cfbc3a34",
     "locked": true,
     "points": 0,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "from scipy.stats import multivariate_normal # Multivariate normal random variable\n",
    "\n",
    "\n",
    "def update_degrees_of_belonging(data, means, covariances,coefficients,k): \n",
    "    cluster_vectors=np.zeros((k,data.shape[0]))   \n",
    "    for i in range(data.shape[0]):\n",
    "        belonging_all=np.sum([coefficients[f]*multivariate_normal.pdf(data[i],means[f],covariances[f]) for f in range(k)])\n",
    "        for t in range(k):\n",
    "            cluster_vectors[t,i]=coefficients[t]*multivariate_normal.pdf(data[i],means[t],covariances[t])/belonging_all\n",
    "    \n",
    "    return cluster_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "48277b6b0a43d4f4f9fe870cd9aa677b",
     "grade": true,
     "grade_id": "cell-cc57f1f24e0499fa",
     "locked": true,
     "points": 0,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "cluster_vectors=update_degrees_of_belonging(data,means,covariances,coefficients,3)\n",
    "plot_GMM(data,means,covariances,3,cluster_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "c2f07a1368467aabf512d5e9e20c6d8b",
     "grade": false,
     "grade_id": "cell-888028517eefa9e3",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Step 3: Set the new cluster parameters\n",
    "\n",
    "Now we want to update the cluster parameters. In contrast to $k$-means, where we only updated the clusters means here we also have to update the covariances and coefficients. Since we use degrees of belonging the formula for updating the means is also slightly different compared to k-means. See the updating formulas below. \n",
    "\n",
    "$$p_{c}=\\frac{N_{c}}{N}$$\n",
    "$$\\mathbf{m}^{(c)} = \\frac{1}{N_c} \\sum_{i=1}^N y_c^{(i)}\\mathbf{x}^{(i)} $$\n",
    "$$\\mathbf{C}^{(c)} = \\frac{1}{N_c} \\sum_{i=1}^N y_c^{(i)}(\\mathbf{x}^{(i)} - \\mathbf{m}^{(c)})(\\mathbf{x}^{(i)} - \\mathbf{m}^{(c)})^T $$\n",
    "where $N_c = \\sum_{i=1}^N y_c^{(i)}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "584d5bf4b74ac964459021e9ad4a8be8",
     "grade": true,
     "grade_id": "cell-1f69dd08c4cd9b59",
     "locked": true,
     "points": 0,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def update_GMM_pars(data, cluster_vectors,k): \n",
    "    means_new = np.zeros((k, data.shape[1]))\n",
    "    covariances_new=np.zeros((k,data.shape[1],data.shape[1]))\n",
    "    coefficients_new=np.zeros(k)\n",
    "    \n",
    "    for i in range(k):\n",
    "        sum_k=np.sum(cluster_vectors[i])\n",
    "        coefficients_new[i]=sum_k/len(data)\n",
    "        means_new[i]=data.T@cluster_vectors[i]/sum_k\n",
    "        for l in range(data.shape[0]):\n",
    "            covariances_new[i]+=np.outer( data[l] - means_new[i], data[l] - means_new[i]) * cluster_vectors[i,l]\n",
    "        covariances_new[i]=covariances_new[i]/sum_k\n",
    "    return means_new,covariances_new,coefficients_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "1fe002a394f8002cf045f10acadfeb1c",
     "grade": true,
     "grade_id": "cell-b9bb4bcce8ecbd23",
     "locked": true,
     "points": 0,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "means,covariances,coefficients=update_GMM_pars(data, cluster_vectors,3)\n",
    "plot_GMM(data,means,covariances,3,cluster_vectors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "6a8bebab03f8b6a71149775b72e73ff3",
     "grade": false,
     "grade_id": "cell-0f4bf81b71d2fc11",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "### Step 4: Put steps 2 and 3 in a loop\n",
    "\n",
    "The last step is to put all the pieces together by initializing the parameters and then putting step 2 and 3 inside a loop. This is very similar to the process in k-means. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "687a25035e8ba630bc335f934ce8cb33",
     "grade": true,
     "grade_id": "cell-1be6cc1e9b97edd8",
     "locked": true,
     "points": 0,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "def GMM_clustering(data,k,num_iters,random_seed=0):\n",
    "    # Step 1: \n",
    "    means,covariances,coefficients = initialize_parameters(data,k,random_seed)\n",
    "    \n",
    "    # This will display a progress bar during GMM execution\n",
    "    f = IntProgress(description=f'GMM (k={k}):',min=0, max=num_iters)\n",
    "    display(f)\n",
    "    for i in range(num_iters):\n",
    "        # Step 2:\n",
    "        cluster_vectors = update_degrees_of_belonging(data, means, covariances,coefficients,k)\n",
    "        # Step 3:\n",
    "        means,covariances,coefficients=update_GMM_pars(data, cluster_vectors,k)\n",
    "        \n",
    "        # Iterate progress bar\n",
    "        f.value +=1\n",
    "    \n",
    "    return means, covariances, cluster_vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "markdown",
     "checksum": "31e22cba74f78ce3cd2bfc1dd2933379",
     "grade": false,
     "grade_id": "cell-5fe73c8270fab14d",
     "locked": true,
     "schema_version": 2,
     "solution": false
    }
   },
   "source": [
    "Now run the code to calculate the final means and covariance matrices and plot the result!\n",
    "In the plot we can clearly see an overlap of the clusters, i.e. some points belonging to in this case 2 clusters. What does this mean and how would this influence the customer segmentation and marketing strategy for the cafe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "nbgrader": {
     "cell_type": "code",
     "checksum": "3ebd56f4db27baef7a3ee323de108e35",
     "grade": true,
     "grade_id": "cell-c3bcf106cc808c3b",
     "locked": true,
     "points": 0,
     "schema_version": 2,
     "solution": false
    }
   },
   "outputs": [],
   "source": [
    "means,covariances,cluster_vectors=GMM_clustering(data,3,50)\n",
    "plot_GMM(data,means,covariances,3,cluster_vectors)\n",
    "print(\"The means are\",means)\n",
    "print(\"The covariance matrices are\",covariances)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
